{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# To help the model converge faster, shrink the intensity values (X) from 0-255 to 0-1\n",
    "X_train = load_data('./data/raw/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/raw/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/raw/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/raw/test-labels.gz', True).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# set inline backend\n",
    "%matplotlib inline\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_NAME=''\n",
    "SUBSCRIPTION_ID=''\n",
    "RESOURCE_GROUP=''\n",
    "LOCATION=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=SUBSCRIPTION_ID, \n",
    "    resource_group=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    "    _location=LOCATION\n",
    ")\n",
    "\n",
    "# create a new workspace or retrieve an existing one\n",
    "ws = ws.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show workspace details\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# write the details of the workspace to a configuration file to the notebook library\n",
    "if not os.path.exists('config.json'):\n",
    "    ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a remote compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# Step 1: name the cluster and set the minimal and maximal number of nodes \n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 3)\n",
    "\n",
    "# Step 2: choose VM size\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "provisioning_config = AmlCompute.provisioning_configuration(\n",
    "    vm_size = vm_size, min_nodes = min_nodes, max_nodes = max_nodes)\n",
    "\n",
    "# create the cluster\n",
    "compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and create a modeling script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data by using get_default_datastore()\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data/raw', target_path='mnist', \n",
    "          overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create the folder\n",
    "folder_training_script = './trial_model_mnist'\n",
    "os.makedirs(folder_training_script, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $folder_training_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "# create three parameters, the location of the data files, and the maximun value of k and the interval\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--kmax', type=int, dest='kmax', default=15, help='max k value')\n",
    "parser.add_argument('--kinterval', type=int, dest='kinterval', default=2, help='k interval')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'mnist')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load the train and test set into numpy arrays\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "\n",
    "#print variable set dimension\n",
    "print(X_train.shape, X_test.shape, sep = '\\n')\n",
    "\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "\n",
    "#print the response variable dimension\n",
    "print( y_train.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Train kNN models with k equals to', range(1,args.kmax,args.kinterval))\n",
    "\n",
    "# generate a wide range of k and find the best models\n",
    "# also create a list to store the evaluation result for each value of k\n",
    "kVals = range(1, args.kmax, args.kinterval)\n",
    "evaluation = []\n",
    "\n",
    "# loop over the models with different parameters to find the one with the lowest error rate\n",
    "for k in kVals:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # use the test dataset for evaluation and append the result to the evaluation list\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    evaluation.append(score)\n",
    "\n",
    "# find the value of k with the best performance\n",
    "i = int(np.argmax(evaluation))\n",
    "print(\"k=%d with best performance with %.2f%% accuracy given current testset\" % (kVals[i], evaluation[i] * 100))\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "run.log('Best_k', kVals[i])\n",
    "run.log('accuracy', evaluation[i])\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# note that the file saved in the outputs folder automatically uploads into the experiment record\n",
    "joblib.dump(value=model, filename='outputs/knn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--kmax': 5,\n",
    "    '--kinterval': 2\n",
    "}\n",
    "\n",
    "#import the Scikit-learn package \n",
    "est = Estimator(source_directory=folder_training_script,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the model, monitor the run, and retrieve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# create an experiment\n",
    "experiment = Experiment(workspace=ws, name='mnist')\n",
    "\n",
    "run = experiment.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor the run\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the result\n",
    "print(run.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amlenv] *",
   "language": "python",
   "name": "conda-env-amlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
